# Read the fourth PDF file
pdf_file4 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Ayivor et al., 2020.pdf"
pdf_text4 <- pdf_text(pdf_file4)
# Preprocess the text
pdf_text_combined4 <- paste(pdf_text4, collapse = " ")
corpus4 <- Corpus(VectorSource(pdf_text_combined4))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, english_stopwords)
corpus4 <- tm_map(corpus4, removeWords, custom_stopwords)
corpus4 <- tm_map(corpus4, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus4)
## Step 4: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.99)  # Adjust the sparsity threshold as needed
## Step 5: Perform LDA
# Set the number of topics
num_topics <- 5  # Adjust the number of topics as needed
# Perform LDA
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Get the results
topics <- tidy(lda_model, matrix = "beta")
## Step 6: Analyze the Topics
# Get top terms per topic
top_terms <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Print top terms for each topic
print(top_terms)
## Step 7: Visualize the topics
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "RAPPAM - Top Terms in Each Topic",
x = "Term",
y = "Beta") +
theme_minimal()
## Step 1: Install and Load Required Packages
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(broom)
library(dplyr)
library(tidytext)
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(ggplot2)
## Step 2: Read the PDF
pdf_file <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Lu et al., 2012.pdf"
pdf_text <- pdf_text(pdf_file)
## Step 3: Preprocess the Text
# Convert the PDF text into a single character vector
pdf_text_combined <- paste(pdf_text, collapse = " ")
# Create a corpus
corpus <- Corpus(VectorSource(pdf_text_combined))
# Stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
custom_stopwords <- c("effectiveness", "ment",
"protected", "area", "areas", "pas", "pa",
"evaluation", "assessment", "methodology",
"rappam", "rapid assessment and prioritization of protected areas management",
"taiwan", "wwf", "khuzestan", "iran", "province", "iucn", "brazilian", "thailand",
"leverington", "hockings", "coelho", "others", "journal", # references
"wwk", "trm", "hcc", "rrm", "dtk", # names of the areas in article 1
"shimbar", "koraii", "dez", "haft shahidan", "karkhe", # names of the areas in  article 2
"pec", #names of the areas in article 3
"study", "studies", "case",
"scores", "score", "results",
"importance", "major")
# Preprocess the corpus: convert to lowercase, remove punctuation, numbers, stopwords, and whitespace
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, english_stopwords)
corpus <- tm_map(corpus, removeWords, custom_stopwords)
#Corpus <- tm_map(Corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, stripWhitespace)
# Read the second PDF file
pdf_file2 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Mohseni et al., 2019.pdf"
pdf_text2 <- pdf_text(pdf_file2)
# Preprocess the text
pdf_text_combined2 <- paste(pdf_text2, collapse = " ")
corpus2 <- Corpus(VectorSource(pdf_text_combined2))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, english_stopwords)
corpus2 <- tm_map(corpus2, removeWords, custom_stopwords)
corpus2 <- tm_map(corpus2, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus2)
# Read the third PDF file
pdf_file3 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Coelho Junior et al., 2020.pdf"
pdf_text3 <- pdf_text(pdf_file3)
# Preprocess the text
pdf_text_combined3 <- paste(pdf_text3, collapse = " ")
corpus3 <- Corpus(VectorSource(pdf_text_combined3))
corpus3 <- tm_map(corpus3, content_transformer(tolower))
corpus3 <- tm_map(corpus3, removePunctuation)
corpus3 <- tm_map(corpus3, removeNumbers)
corpus3 <- tm_map(corpus3, removeWords, english_stopwords)
corpus3 <- tm_map(corpus3, removeWords, custom_stopwords)
corpus3 <- tm_map(corpus3, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus3)
# Read the fourth PDF file
pdf_file4 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Ayivor et al., 2020.pdf"
pdf_text4 <- pdf_text(pdf_file4)
# Preprocess the text
pdf_text_combined4 <- paste(pdf_text4, collapse = " ")
corpus4 <- Corpus(VectorSource(pdf_text_combined4))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, english_stopwords)
corpus4 <- tm_map(corpus4, removeWords, custom_stopwords)
corpus4 <- tm_map(corpus4, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus4)
## Step 4: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.99)  # Adjust the sparsity threshold as needed
## Step 5: Perform LDA
# Set the number of topics
num_topics <- 5  # Adjust the number of topics as needed
# Perform LDA
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Get the results
topics <- tidy(lda_model, matrix = "beta")
## Step 6: Analyze the Topics
# Get top terms per topic
top_terms <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Print top terms for each topic
print(top_terms)
## Step 7: Visualize the topics
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "RAPPAM - Top Terms in Each Topic",
x = "Term",
y = "Beta") +
theme_minimal()
## Step 1: Install and Load Required Packages
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(broom)
library(dplyr)
library(tidytext)
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(ggplot2)
## Step 2: Read the PDF
pdf_file <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Lu et al., 2012.pdf"
pdf_text <- pdf_text(pdf_file)
## Step 3: Preprocess the Text
# Convert the PDF text into a single character vector
pdf_text_combined <- paste(pdf_text, collapse = " ")
# Create a corpus
corpus <- Corpus(VectorSource(pdf_text_combined))
# Stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
custom_stopwords <- c("effectiveness", "ment",
"protected", "area", "areas", "pas", "pa",
"evaluation", "assessment", "methodology",
"rappam", "rapid assessment and prioritization of protected areas management",
"taiwan", "wwf", "khuzestan", "iran", "province", "iucn", "brazilian", "thailand", "ghana",
"leverington", "hockings", "coelho", "others", "journal", # references
"wwk", "trm", "hcc", "rrm", "dtk", # names of the areas in article 1
"shimbar", "koraii", "dez", "haft shahidan", "karkhe", # names of the areas in  article 2
"pec", #names of the areas in article 3
"kalakpa", "kogyae", #names of the areas in artcile 4
"study", "studies", "case",
"scores", "score", "results",
"importance", "major")
# Preprocess the corpus: convert to lowercase, remove punctuation, numbers, stopwords, and whitespace
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, english_stopwords)
corpus <- tm_map(corpus, removeWords, custom_stopwords)
#Corpus <- tm_map(Corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, stripWhitespace)
# Read the second PDF file
pdf_file2 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Mohseni et al., 2019.pdf"
pdf_text2 <- pdf_text(pdf_file2)
# Preprocess the text
pdf_text_combined2 <- paste(pdf_text2, collapse = " ")
corpus2 <- Corpus(VectorSource(pdf_text_combined2))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, english_stopwords)
corpus2 <- tm_map(corpus2, removeWords, custom_stopwords)
corpus2 <- tm_map(corpus2, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus2)
# Read the third PDF file
pdf_file3 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Coelho Junior et al., 2020.pdf"
pdf_text3 <- pdf_text(pdf_file3)
# Preprocess the text
pdf_text_combined3 <- paste(pdf_text3, collapse = " ")
corpus3 <- Corpus(VectorSource(pdf_text_combined3))
corpus3 <- tm_map(corpus3, content_transformer(tolower))
corpus3 <- tm_map(corpus3, removePunctuation)
corpus3 <- tm_map(corpus3, removeNumbers)
corpus3 <- tm_map(corpus3, removeWords, english_stopwords)
corpus3 <- tm_map(corpus3, removeWords, custom_stopwords)
corpus3 <- tm_map(corpus3, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus3)
# Read the fourth PDF file
pdf_file4 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Ayivor et al., 2020.pdf"
pdf_text4 <- pdf_text(pdf_file4)
# Preprocess the text
pdf_text_combined4 <- paste(pdf_text4, collapse = " ")
corpus4 <- Corpus(VectorSource(pdf_text_combined4))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, english_stopwords)
corpus4 <- tm_map(corpus4, removeWords, custom_stopwords)
corpus4 <- tm_map(corpus4, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus4)
## Step 4: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.99)  # Adjust the sparsity threshold as needed
## Step 5: Perform LDA
# Set the number of topics
num_topics <- 5  # Adjust the number of topics as needed
# Perform LDA
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Get the results
topics <- tidy(lda_model, matrix = "beta")
## Step 6: Analyze the Topics
# Get top terms per topic
top_terms <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Print top terms for each topic
print(top_terms)
## Step 7: Visualize the topics
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "RAPPAM - Top Terms in Each Topic",
x = "Term",
y = "Beta") +
theme_minimal()
## Step 1: Install and Load Required Packages
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(broom)
library(dplyr)
library(tidytext)
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(ggplot2)
## Step 2: Read the PDF
pdf_file <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Lu et al., 2012.pdf"
pdf_text <- pdf_text(pdf_file)
## Step 3: Preprocess the Text
# Convert the PDF text into a single character vector
pdf_text_combined <- paste(pdf_text, collapse = " ")
# Create a corpus
corpus <- Corpus(VectorSource(pdf_text_combined))
# Stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
custom_stopwords <- c("effectiveness", "ment",
"protected", "area", "areas", "pas", "pa",
"evaluation", "assessment", "methodology",
"rappam", "rapid assessment and prioritization of protected areas management",
"taiwan", "wwf", "khuzestan", "iran", "province", "iucn", "brazilian", "thailand", "ghana",
"leverington", "hockings", "coelho", "others", "journal", # references
"wwk", "trm", "hcc", "rrm", "dtk", # names of the areas in article 1
"shimbar", "koraii", "dez", "haft shahidan", "karkhe", # names of the areas in  article 2
"pec", #names of the areas in article 3
"kalakpa", "kogyae", #names of the areas in artcile 4
"study", "studies", "case",
"scores", "score", "results",
"importance", "major", "include")
# Preprocess the corpus: convert to lowercase, remove punctuation, numbers, stopwords, and whitespace
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, english_stopwords)
corpus <- tm_map(corpus, removeWords, custom_stopwords)
#Corpus <- tm_map(Corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, stripWhitespace)
# Read the second PDF file
pdf_file2 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Mohseni et al., 2019.pdf"
pdf_text2 <- pdf_text(pdf_file2)
# Preprocess the text
pdf_text_combined2 <- paste(pdf_text2, collapse = " ")
corpus2 <- Corpus(VectorSource(pdf_text_combined2))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, english_stopwords)
corpus2 <- tm_map(corpus2, removeWords, custom_stopwords)
corpus2 <- tm_map(corpus2, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus2)
# Read the third PDF file
pdf_file3 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Coelho Junior et al., 2020.pdf"
pdf_text3 <- pdf_text(pdf_file3)
# Preprocess the text
pdf_text_combined3 <- paste(pdf_text3, collapse = " ")
corpus3 <- Corpus(VectorSource(pdf_text_combined3))
corpus3 <- tm_map(corpus3, content_transformer(tolower))
corpus3 <- tm_map(corpus3, removePunctuation)
corpus3 <- tm_map(corpus3, removeNumbers)
corpus3 <- tm_map(corpus3, removeWords, english_stopwords)
corpus3 <- tm_map(corpus3, removeWords, custom_stopwords)
corpus3 <- tm_map(corpus3, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus3)
# Read the fourth PDF file
pdf_file4 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Ayivor et al., 2020.pdf"
pdf_text4 <- pdf_text(pdf_file4)
# Preprocess the text
pdf_text_combined4 <- paste(pdf_text4, collapse = " ")
corpus4 <- Corpus(VectorSource(pdf_text_combined4))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, english_stopwords)
corpus4 <- tm_map(corpus4, removeWords, custom_stopwords)
corpus4 <- tm_map(corpus4, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus4)
## Step 4: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.99)  # Adjust the sparsity threshold as needed
## Step 5: Perform LDA
# Set the number of topics
num_topics <- 5  # Adjust the number of topics as needed
# Perform LDA
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Get the results
topics <- tidy(lda_model, matrix = "beta")
## Step 6: Analyze the Topics
# Get top terms per topic
top_terms <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Print top terms for each topic
print(top_terms)
## Step 7: Visualize the topics
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "RAPPAM - Top Terms in Each Topic",
x = "Term",
y = "Beta") +
theme_minimal()
## Step 1: Install and Load Required Packages
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(broom)
library(dplyr)
library(tidytext)
library(pdftools)
library(tm)
library(textmineR)
library(topicmodels)
library(ggplot2)
## Step 2: Read the PDF
pdf_file <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Lu et al., 2012.pdf"
pdf_text <- pdf_text(pdf_file)
## Step 3: Preprocess the Text
# Convert the PDF text into a single character vector
pdf_text_combined <- paste(pdf_text, collapse = " ")
# Create a corpus
corpus <- Corpus(VectorSource(pdf_text_combined))
# Stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
custom_stopwords <- c("effectiveness", "ment",
"protected", "area", "areas", "pas", "pa",
"evaluation", "assessment", "methodology",
"rappam", "rapid assessment and prioritization of protected areas management",
"taiwan", "wwf", "khuzestan", "iran", "province", "iucn", "brazilian", "thailand", "ghana",
"leverington", "hockings", "coelho", "others", "journal", "fig", # references
"wwk", "trm", "hcc", "rrm", "dtk", # names of the areas in article 1
"shimbar", "koraii", "dez", "haft shahidan", "karkhe", # names of the areas in  article 2
"pec", #names of the areas in article 3
"kalakpa", "kogyae", #names of the areas in article 4
"study", "studies", "case",
"scores", "score", "results",
"importance", "major", "include")
# Preprocess the corpus: convert to lowercase, remove punctuation, numbers, stopwords, and whitespace
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, english_stopwords)
corpus <- tm_map(corpus, removeWords, custom_stopwords)
#Corpus <- tm_map(Corpus, stemDocument, language = "en")
corpus <- tm_map(corpus, stripWhitespace)
# Read the second PDF file
pdf_file2 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Mohseni et al., 2019.pdf"
pdf_text2 <- pdf_text(pdf_file2)
# Preprocess the text
pdf_text_combined2 <- paste(pdf_text2, collapse = " ")
corpus2 <- Corpus(VectorSource(pdf_text_combined2))
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, english_stopwords)
corpus2 <- tm_map(corpus2, removeWords, custom_stopwords)
corpus2 <- tm_map(corpus2, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus2)
# Read the third PDF file
pdf_file3 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Coelho Junior et al., 2020.pdf"
pdf_text3 <- pdf_text(pdf_file3)
# Preprocess the text
pdf_text_combined3 <- paste(pdf_text3, collapse = " ")
corpus3 <- Corpus(VectorSource(pdf_text_combined3))
corpus3 <- tm_map(corpus3, content_transformer(tolower))
corpus3 <- tm_map(corpus3, removePunctuation)
corpus3 <- tm_map(corpus3, removeNumbers)
corpus3 <- tm_map(corpus3, removeWords, english_stopwords)
corpus3 <- tm_map(corpus3, removeWords, custom_stopwords)
corpus3 <- tm_map(corpus3, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus3)
# Read the fourth PDF file
pdf_file4 <- "C:/Users/derue/Documents/BAM/Thesis/Codes/RAPPAM/Ayivor et al., 2020.pdf"
pdf_text4 <- pdf_text(pdf_file4)
# Preprocess the text
pdf_text_combined4 <- paste(pdf_text4, collapse = " ")
corpus4 <- Corpus(VectorSource(pdf_text_combined4))
corpus4 <- tm_map(corpus4, content_transformer(tolower))
corpus4 <- tm_map(corpus4, removePunctuation)
corpus4 <- tm_map(corpus4, removeNumbers)
corpus4 <- tm_map(corpus4, removeWords, english_stopwords)
corpus4 <- tm_map(corpus4, removeWords, custom_stopwords)
corpus4 <- tm_map(corpus4, stripWhitespace)
# Combine the preprocessed text with the existing corpus
corpus <- c(corpus, corpus4)
## Step 4: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.99)  # Adjust the sparsity threshold as needed
## Step 5: Perform LDA
# Set the number of topics
num_topics <- 5  # Adjust the number of topics as needed
# Perform LDA
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Get the results
topics <- tidy(lda_model, matrix = "beta")
## Step 6: Analyze the Topics
# Get top terms per topic
top_terms <- topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Print top terms for each topic
print(top_terms)
## Step 7: Visualize the topics
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "RAPPAM - Top Terms in Each Topic",
x = "Term",
y = "Beta") +
theme_minimal()
